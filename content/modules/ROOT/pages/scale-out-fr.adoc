# Déploiement à l'échelle (scale-out) avec un nœud provisionné par l'opérateur Metal3/Baremetal Cluster

Sur le serveur *hyperviseur*, créez une machine virtuelle vide supplémentaire pour héberger le deuxième noeud de calcul :

[source,bash,role=execute]
----
sudo -i
cd /var/lib/libvirt/images
qemu-img create -f qcow2 /var/lib/libvirt/images/osp-compute-1.qcow2 150G
virt-install --virt-type kvm --ram 6144 --vcpus 2 --cpu=host-passthrough --os-variant rhel8.4 --disk path=/var/lib/libvirt/images/osp-compute-1.qcow2,device=disk,bus=virtio,format=qcow2 --network network:ocp4-provisioning,mac="de:ad:be:ef:00:07" --network network:ocp4-net --boot hd,network --noautoconsole --vnc --name osp-compute1 --noreboot
virsh start osp-compute1
----

Ajoutez le nouveau noeud dans l'outil virtualbmc afin qu'il puisse être géré via IPMI

[source,bash,role=execute]
----
iptables -A LIBVIRT_INP -p udp --dport 6237 -j ACCEPT
vbmc add --username admin --password redhat --port 6237 --address 192.168.123.1 --libvirt-uri qemu:///system osp-compute1
vbmc start osp-compute1
----

Le nouvel hôte devrait être visible :
[source,bash]
----
[root@hypervisor ~]# vbmc list
+--------------+---------+---------------+------+
| Domain name  | Status  | Address       | Port |
+--------------+---------+---------------+------+
| ocp4-bastion | running | 192.168.123.1 | 6230 |
| ocp4-master1 | running | 192.168.123.1 | 6231 |
| ocp4-master2 | running | 192.168.123.1 | 6232 |
| ocp4-master3 | running | 192.168.123.1 | 6233 |
| ocp4-worker1 | running | 192.168.123.1 | 6234 |
| ocp4-worker2 | running | 192.168.123.1 | 6235 |
| ocp4-worker3 | running | 192.168.123.1 | 6236 |
| osp-compute1 | running | 192.168.123.1 | 6237 |
+--------------+---------+---------------+------+
----

De retour sur le serveur **bastion** :

Cloner le référentiel de fichiers

Dans le terminal sur le bastion, assurez-vous que vous êtes dans le bon répertoire  :

[source,bash,role=execute]
----
cd labrepo/content/files
----

Le BMO gère par défaut les CR BareMetalHost dans le namespace openshift-machine-api. Vous devez mettre à jour le CR de provisionnement pour surveiller tous les namespaces:

[source,bash,role=execute]
----
oc patch provisioning provisioning-configuration --type merge -p '{"spec":{"watchAllNamespaces": true }}'
----

Créer un hôte Baremetal 
[source,bash,role=execute]
----
oc apply -f osp-ng-osp-compute1-bmh.yaml -n openshift-machine-api
----

Attendez que l'hôte de base soit dans l'état Disponible. Le bmh passera d'abord à l'enregistrement (registering), puis à l'inspection (inspecting) et enfin à l'état disponible (available). Ce processus peut prendre environ 4 minutes.
[source,bash,role=execute]
----
oc get bmh -n openshift-machine-api -w
----
.Exemple de sortie :

[source,bash]
----
oc get bmh -n openshift-machine-api
NAME           STATE                    CONSUMER                   ONLINE   ERROR   AGE
master1        externally provisioned   ocp-49jkw-master-0         true             12h
master2        externally provisioned   ocp-49jkw-master-1         true             12h
master3        externally provisioned   ocp-49jkw-master-2         true             12h
osp-compute1   available                                           false            7m39s
worker1        provisioned              ocp-49jkw-worker-0-k9rl2   true             12h
worker2        provisioned              ocp-49jkw-worker-0-xm9fs   true             12h
worker3        provisioned              ocp-49jkw-worker-0-czfxj   true             12h
----
Remarque : appuyez sur Ctrl + C pour quitter la commande en attente

Étiquetez l'hôte baremetal avec **app:openstack** afin qu'il puisse être utilisé par le CR openstackbaremetalset:
[source,bash,role=execute]
----
oc label BareMetalHost osp-compute1 -n openshift-machine-api app=openstack
----

Déployer le Dataplane

Enfin, appliquez :
[source,bash,role=execute]
----
oc apply -f osp-ng-dataplane-node-set-deploy-scale-out.yaml
oc apply -f osp-ng-dataplane-deployment-scale-out.yaml
----

Un pod de provisionnement sera exécuté pour extraire l'image RHEL edpm et effectuer le provisionnement dans le nœud :

[source,bash,role=execute]
----
oc get pods -n openstack
----

.Exemple de sortie
[source,bash]
----
NAME                                                              READY   STATUS      RESTARTS   AGE
[...]
reboot-os-openstack-edpm-ipam-openstack-edpm-ipam-scqq9           0/1     Completed   0          111m
run-os-openstack-edpm-ipam-openstack-edpm-ipam-zs4dk              0/1     Completed   0          111m
scale-out-provisioned-provisionserver-openstackprovisionse2csnp   1/1     Running     0          2m18s
ssh-known-hosts-openstack-edpm-ipam-67lt8                         0/1     Completed   0          111m
validate-network-openstack-edpm-ipam-openstack-edpm-ipam-r22jq    0/1     Completed   0          112m
[...]
----

Le nœud passera de l'état disponible (available) à l'état de provisionnement (provisionning) :
[source,bash,role=execute]
----
oc get bmh -n openshift-machine-api
----

.Exemple de sortie
[source,bash]
----
NAME 
NAME           STATE          CONSUMER                   ONLINE   ERROR   AGE
master1        provisioned    ocp-df4n7-master-0         true             3h48m
master2        provisioned    ocp-df4n7-master-1         true             3h48m
master3        provisioned    ocp-df4n7-master-2         true             3h48m
osp-compute1   provisioning   scale-out-provisioned      true             48m
worker1        provisioned    ocp-df4n7-worker-0-bxrmx   true             3h48m
worker2        provisioned    ocp-df4n7-worker-0-z57z6   true             3h48m
worker3        provisioned    ocp-df4n7-worker-0-f5ndx   true             3h48m
----

Après le provisionnement du nœud, le déploiement se déroulera de la même manière que dans la section pré-provisionnée :

Vous pouvez afficher les journaux Ansible pendant l'exécution du déploiement :

[source,bash,role=execute]
----
oc logs -l app=openstackansibleee -f --max-log-requests 10
----

.Exemple de sortie
[source,bash,role=execute]
----
(...)
PLAY RECAP *********************************************************************
edpm-compute-1             : ok=53   changed=26   unreachable=0    failed=0    skipped=54   rescued=0    ignored=0
----

Ctrl-C pour quitter.

Vérifiez que le plan de données est déployé.

REMARQUE : cette opération prend plusieurs minutes.

----
oc get openstackdataplanedeployment
----

Répétez la requête jusqu'à ce que vous voyiez ce qui suit :

.Exemple de sortie
[source,bash,role=execute]
----
NAME                  STATUS   MESSAGE
openstack-scale-out-provisioned   True     Setup Complete
----

[source,bash,role=execute]
----
oc get openstackdataplanenodeset
----

Répétez la requête jusqu'à ce que vous voyiez ce qui suit :

[source,bash,role=execute]
----
NAME                  STATUS   MESSAGE
scale-out-provisioned   True     NodeSet Ready
----

Mappez les nouveaux nœuds de calcul à la cellule de calcul à laquelle ils sont connectés :
[source,bash,role=execute]
----
oc rsh nova-cell0-conductor-0 nova-manage cell_v2 discover_hosts --verbose
----

Le nœud edp-compute-1 doit être visible dans la liste des services de calcul :
[source,bash,role=execute]
----
oc rsh -n openstack openstackclient
openstack compute service list
----

Si vous avez besoin d’accéder à votre nœud de calcul provisionné :

Récupérer les ipsets dans l'espace de noms openstack

[source,bash,role=execute]
----
oc get ipset -n openstack
NAME             READY   MESSAGE          RESERVATION
edpm-compute-0   True    Setup complete
edpm-compute-1   True    Setup complete
----

Décrivez le nœud provisionné **edpm-compute-1** :
[source,bash,role=execute]
----
oc describe ipset edpm-compute-1 -n openstack
----

Vous obtiendrez l'adresse du plan de contrôle dans les propriétés de réservation :

[source,bash]
----
Output
[...]
  Observed Generation:     1
  Reservations:
    Address:     172.22.0.101
    Cidr:        172.22.0.0/24
    Dns Domain:  ctlplane.aio.example.com
    Gateway:     172.22.0.1
    Mtu:         1500
    Network:     ctlplane
    Routes:
      Destination:  0.0.0.0/0
      Nexthop:      172.22.0.1
[...]
----

Enfin, en cas de besoin, vous pouvez vous connecter en ssh à edp-compute1 en utilisant l'adresse de la sortie précédente :

[source,bash]
----
ssh -i /root/.ssh/id_rsa_compute cloud-admin@172.22.0.101
----
